[{"id":"ad53f22647fa1ab58dafd47d2fbb69ce","title":"PostgreSQL数据的存储基础知识","content":"PostgerSQL对象标识符对象标识符是PostSQL很重要的一个特征，需要熟练掌握。\nOIDOID 是 PostgreSQL 内部用于标识数据库对象(数据库，表**，视图，**存储过程等等)的标识符，用4个字节的无符号整数表示。它是PostgreSQL大部分系统表的主键。\n类型oid表示一个对象标识符。 也有多个oid的别名类型：regproc,regprocedure, regoper, regoperator,regclass, regtype, regrole,regnamespace, regconfig, 和regdictionary。\n\nOID的别名类型除了特定的输入和输出例程之外没有别的操作。这些例程可以接受并显示系统对象的符号名，而不是类型oid使用的原始数字值。别名类型使查找对象的OID值变得简单。例如，要检查与一个表course有关的pg_attribute行，你可以写：\nSELECT * FROM pg_attribute WHERE attrelid &#x3D; &#39;course&#39;::regclass;\n\n\nOID 在系统表中通常是作为隐藏列存在的，它是以整个PostgreSQL数据库实例(Database Cluster)的范围内统一分配。因为只有四个字节，因此，在大型数据库中它并不足以提供数据库范围内的唯一性，甚至在一些大型的表中也无法提供表范围内的唯一性。\nOID 在旧版本中还可以用于标识元组，对于没有主键，重复的行，此时 OID 作为唯一 ID，则可以根据它进行删除指定行数据。我们之前创建表时，default_with_oids 默认是关闭的。在老版本中执行 create table 语句时可以指定开启 OID。\ncreate table foo (\n    id integer,\n    content text\n) with oids;\n\n不过从 Postgres 12 开始，删除了将 OID 用作表上的可选系统列。将无法再使用：\n\nCREATE TABLE … WITH OIDS 命令\ndefault_with_oids (boolean) 相容性设定\n\n数据类型OID保留在Postgres 12中。您可以显式创建类型的列OID。\nXID事务ID：\n\n由32位组成，这就有可能造成事务ID回卷的问题，具体参考文档\n顺序产生，依次递增\n没有数据变更，如INSERT、UPDATE、DELETE等操作，在当前会话中，事务ID不会改变\n\n数据库系统中使用的数据类型为 xmin 和 xmax。\n\nxmin 存储的是产生这个元组的事务ID，可能是insert或者update语句\nxmax 存储的是删除或者锁定这个元组的XID\n\n简单示例如下：\nselect id, xmin, xmax from course;\n\n\n当 PostgreSQL的XID 到达40亿，会造成溢出，从而新的XID 为0。而按照 PostgreSQL的MVCC 机制实现，之前的事务就可以看到这个新事务创建的元组，而新事务不能看到之前事务创建的元组，这违反了事务的可见性。具体参考文档\nCIDCID 名为命令标识符，PG 每个表都包含一些系统字段，关于 CID 用到的数据类型为 cmax 和 cmin。\n\ncmin:插入该元组的命令在插入事务中的命令标识（从0开始累加）\ncmax:删除该元组的命令在插入事务中的命令标识（从0开始累加）\n\ncmin和cmax用于判断同一个事务内的其他命令导致的行版本变更是否可见。如果一个事务内的所有命令严格顺序执行，那么每个命令总能看到之前该事务内的所有变更，不需要使用命令标识。\n简单示例如下：\nselect id, xmin, xmax,cmin,cmax from course;\n\n\nTIDTID 称为元组标识符（行标识符），一个元组ID是一个（块号，块内元组索引）对，它标识了行在它的表中的物理位置。\n简单示例如下：\nselect ctid,id, xmin, xmax,cmin,cmax from course;\n\n\n了解完上述四大标识符后，我们接着来学习 PostgreSQL 中数据到底是怎么存储的。\n​\nPostgreSQL数据存储关于数据存储，我们都知道数据是存在数据库中的某个数据表中，每条数据记录对应数据表中的某一行，所以我们从上至下来查看各层次结构的数据存储。\nPGDATA目录结构PGDATA 是 PostgreSQL 用来存放所有数据的地方。\n关于 PGDATA 的设置，可以先执行下述命令。\npostgres&#x3D;# show data_directory;\n       data_directory        \n-----------------------------\n &#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data\n(1 row)\n\n接下来我们来看一下 PGDATA 文件夹中有哪些文件，首先打开命令行窗口，然后进入到上述目录。\nMacBook-Pro 12 % cd &#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data\ncd: permission denied: &#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data\n\n如果遇到上述问题，则执行如下命令，尝试使用 sudo 模拟 postgresql 用户登录：\nMacBook-Pro 12 % sudo -u postgres -i\n\nThe default interactive shell is now zsh.\nTo update your account to use zsh, please run &#96;chsh -s &#x2F;bin&#x2F;zsh&#96;.\nFor more details, please visit https:&#x2F;&#x2F;support.apple.com&#x2F;kb&#x2F;HT208050.\n\n接着执行如下命令：\ntree -FL 1 &#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data\n\n&#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data\n├── PG_VERSION\n├── base&#x2F;\n├── current_logfiles\n├── global&#x2F;\n├── log&#x2F;\n├── pg_commit_ts&#x2F;\n├── pg_dynshmem&#x2F;\n├── pg_hba.conf\n├── pg_ident.conf\n├── pg_logical&#x2F;\n├── pg_multixact&#x2F;\n├── pg_notify&#x2F;\n├── pg_replslot&#x2F;\n├── pg_serial&#x2F;\n├── pg_snapshots&#x2F;\n├── pg_stat&#x2F;\n├── pg_stat_tmp&#x2F;\n├── pg_subtrans&#x2F;\n├── pg_tblspc&#x2F;\n├── pg_twophase&#x2F;\n├── pg_wal&#x2F;\n├── pg_xact&#x2F;\n├── postgresql.auto.conf\n├── postgresql.conf\n├── postmaster.opts\n└── postmaster.pid\n\n介绍几个常见的文件夹：\n\nbase/：存储 database 数据（除了指定其他表空间的），子目录的名字为该数据库在 pg_database里的 OID。\npostgresql.conf：postgresql 配置文件\n\ndatabase数据存储上文提到在 base&#x2F; 目录下存放着每个 database 数据，其中文件名我们叫做 dboid。\n由于 OID 是系统表的隐藏列，因此查看系统表中数据库对象的OID时，必须在SELECT语句中显式指定。我们进入 postgres 命令行窗口，执行下述命令：\npostgres&#x3D;# select oid,datname from pg_database;\n  oid  |  datname  \n-------+-----------\n 13635 | postgres\n     1 | template1\n 13634 | template0\n 16395 | mydb\n 16396 | dvdrental\n 16399 | testdb\n(6 rows)\nselect oid,relname from pg_class order by oid;\n\n我们可以在 PGDATA 文件夹下的 base 目录下看到 oid\nMacBook-Pro:base postgres$ ls &#x2F;Library&#x2F;PostgreSQL&#x2F;12&#x2F;data&#x2F;base\n1\t13634\t13635\t16395\t16396\t16399\n\n从上述内容可知 postgres 数据库相关的数据存储在 PGDATA/base/13635 目录里面。\ntable数据存储上文我们定位到数据库的存储位置，接着我们来定位数据表的位置。\n每一张表的数据（大部分）又是放在 $PGDATA/base/&#123;dboid&#125;/&#123;relfilenode&#125; 这个文件里面，relfilenode一般情况下和和tboid一致，但有些情况下也会变化，如TRUNCATE、REINDEX、CLUSTER以及某些形式的ALTER TABLE。\nCREATE TABLE public.cities (\n\tcity varchar(80) NOT NULL,\n\t&quot;location&quot; point NULL,\n\tCONSTRAINT cities_pkey PRIMARY KEY (city)\n);\n\npostgres&#x3D;# select oid,relfilenode from pg_class where relname &#x3D; &#39;cities&#39;;\n  oid  | relfilenode \n-------+-------------\n 16475 |       16475\n(1 row)\n\ninsert into cities values(&#39;北京&#39;,null);\ninsert into cities values(&#39;上海&#39;,null);\n\ntruncate cities ;\n\npostgres&#x3D;# select oid,relfilenode from pg_class where relname &#x3D; &#39;cities&#39;;\n  oid  | relfilenode \n-------+-------------\n 16475 |       16480\n(1 row)\n\nSELECT * FROM pg_attribute WHERE attrelid &#x3D; &#39;course&#39;::regclass;\n\n除了上述 SQL 语句，我们还可以通过系统函数 pg_relation_filepath 来查看指定表的文件存储位置。\npostgres&#x3D;# select pg_relation_filepath(&#39;cities&#39;);\n pg_relation_filepath \n----------------------\n base&#x2F;13635&#x2F;16480\n(1 row)\n\n当查看 PGDATA/base/13635/ 目录时，会发现 16480 的文件夹，除此之外还会发现有些文件命名为 relfilenode_fsm、relfilenode_vm、relfilenode_init， 关于 16480 通常会有三种文件：16480、16480_fsm、16480_vm，分别是该数据库对应表的数据或索引文件、其对应的空闲空间映射文件、其对应的可见性映射文件。\n如果数据文件过大，那么会怎么命名呢？\n在表或者索引超过1GB之后，它就被划分成1GB大小的段。 第一个段的文件名和文件节点相同，随后的段被命名为 filenode.1、filenode.2等等。这样的安排避免了在某些有文件大小限制的平台上的问题。\npostgres&#x3D;# create table bigdata(id int,name varchar(64));\n\n\npostgres&#x3D;# insert into bigdata select generate_series(1,20000000) as key, md5(random()::text);\n\n\npostgres&#x3D;# select pg_relation_filepath(&#39;bigdata&#39;);\n pg_relation_filepath \n----------------------\n base&#x2F;13635&#x2F;16486\n(1 row)\n\n#切换命令行界面\nMacBook-Pro:base postgres$ ls 13635 |grep 16486\n16486\n16486.1\n16486_fsm  \n\n元组数据存储上文我们提到 table 存储时，每个数据文件（堆文件、索引文件）可存储 1G 的容量，每个文件内部又是有若干个固定的页组成。页的默认大小为8192字节(8KB)。单个表文件中的这些页(Page)从0开始进行顺序编号，这些编号也称为“块编号(Block Numbers)”。如果第一页空间已经被数据填满，则 postgres 会立刻重新在文件末尾(即已填满页的后面)添加一个新的空白页，用于继续存储数据，一直持续这个过程，直到当前表文件大小达到 1GB位置。若文件达到1GB，则重新创建一个新的表文件，然后重复上面的这个过程。\n每个页的内部又由一个页文件头(Page Header)、若干行指针(Line Pointer)、若干个元组数据(Heaple Tuple)组成。因为每个文件默认大小为 1GB，页大小为 8kb，则每个文件大概有 131072 个页。\n首先来看一下页面结构。\n\n其中：\n\npage header: 24 字节，存储 page 的基本信息，包括 pd_lsn、pd_checksum、pd_special… pd_lsn: 存储最近改变该页面的xlog。 pd_checksum：存储页面校验和。 pd_lower，pd_upper：pd_lower指向行指针（line pointer）的尾部，pd_upper指向最后那个元组。 pd_special: 索引页面中使用，它指向特殊空间的开头。 pd_flags：用以设置位标志。 pd_pagesize_version：页面大小及页面版本号。 pd_prune_xid：可删除的旧 XID，如果没有则为零。 复制代码\n\nline pointe：行指针，4 bytes，形为 (offset, length) 的二元组，指向相关 tuple\n\nheap tuple: 用来存储 row 的数据，注意元组是从页面的尾部向前堆积的，元组和行指针之间的是数据页的空闲空间。\n\n空白处：未申请空间，新的 line point 从其首端申请，新的 tuple 从其尾端申请\n\n\n因此我们找 row 的数据需要知道哪一个 page，page 的哪一个 item， (page_index, item_index)， 通常称它为 CTID(ItemPointer)， 我们可以通过下面语句查看每一列的 CTID：\nselect ctid,* from course;\n\n查询结果如下所示：\n\n关于元组结构以及数据变化的详解讲解，可以参考本文。\n​\n扩展schemaPostgreSQL 除了默认的 public schema 之外，还有两个比较重的系统 schema：information_schema 与pg_catalog。\n通过查看 pg_catalog.pg_namespace 来查看当前数据库中全部的 schema。\npostgres&#x3D;# select * from pg_catalog.pg_namespace ;\n  oid  |      nspname       | nspowner |               nspacl                \n-------+--------------------+----------+-------------------------------------\n    99 | pg_toast           |       10 | \n 12314 | pg_temp_1          |       10 | \n 12315 | pg_toast_temp_1    |       10 | \n    11 | pg_catalog         |       10 | &#123;postgres&#x3D;UC&#x2F;postgres,&#x3D;U&#x2F;postgres&#125;\n  2200 | public             |       10 | &#123;postgres&#x3D;UC&#x2F;postgres,&#x3D;UC&#x2F;postgres&#125;\n 13335 | information_schema |       10 | &#123;postgres&#x3D;UC&#x2F;postgres,&#x3D;U&#x2F;postgres&#125;\n(6 rows)\n\n我们创建的表、视图、索引等默认都在 public 下。\ninformation_schema 是方便用户查看表／视图／函数信息提供的，它大多是视图。\nselect * from information_schema.&quot;tables&quot;;\n\n\npg_catalog 包含系统表和所有内置数据类型、函数、操作符。pg_catalog 下有很多系统表，比如说 pg_class、pg_attribute、pg_authid等。\n","slug":"postgreSql","date":"2023-03-28T08:23:16.000Z","categories_index":"","tags_index":"postgreSQL","author_index":"Sh1mwww"},{"id":"537e66c74d7eb8b9f47d0409dbb02915","title":"在ES的PostgreSql中使用JSONB","content":"引言​\n通过本文可掌握在pg数据库中如何正确使用json字段，如何进行数据查询，在where子查询中如何使用，以及对json值进行聚合查询使用.\n​\n概述JSON 代表 JavaScript Object Notation。JSON是开放的标准格式，由key-value对组成。JSON的主要用于在服务器与web应用之间传输数据。新建表如下：\nCREATE TABLE &quot;public&quot;.&quot;biz_orders&quot; (  &quot;ID&quot; int8 NOT NULL DEFAULT nextval(&#39;&quot;biz_orders_ID_seq&quot;&#39;::regclass),\n  &quot;info&quot; json NOT NULL\n);\n\n表初始化语句：\nINSERT INTO &quot;biz_orders&quot;(&quot;ID&quot;, &quot;info&quot;) VALUES (1, &#39;&#123;&quot;name&quot;:&quot;张三&quot;,&quot;items&quot;:&#123;&quot;product&quot;:&quot;啤酒&quot;,&quot;qty&quot;:6&#125;&#125;&#39;);\nINSERT INTO &quot;biz_orders&quot;(&quot;ID&quot;, &quot;info&quot;) VALUES (2, &#39;&#123;&quot;name&quot;:&quot;李四&quot;,&quot;items&quot;:&#123;&quot;product&quot;:&quot;辣条&quot;,&quot;qty&quot;:8&#125;&#125;&#39;);\nINSERT INTO &quot;biz_orders&quot;(&quot;ID&quot;, &quot;info&quot;) VALUES (3, &#39;&#123;&quot;name&quot;:&quot;王五&quot;,&quot;items&quot;:&#123;&quot;product&quot;:&quot;苹果&quot;,&quot;qty&quot;:18&#125;&#125;&#39;);\nINSERT INTO &quot;biz_orders&quot;(&quot;ID&quot;, &quot;info&quot;) VALUES (4, &#39;&#123;&quot;name&quot;:&quot;赵一&quot;,&quot;items&quot;:&#123;&quot;product&quot;:&quot;香蕉&quot;,&quot;qty&quot;:20&#125;&#125;&#39;);\n\n​\n使用1、简单查询\nselect * from biz_orders;\n\n\n2、查询使用-&gt;操作符，查询json中所有顾客作为键\nSELECT info -&gt; &#39;name&#39; AS customer FROM biz_orders;\n\n\n3、下面使用-&gt;&gt;操作获取所有顾客姓名作为值\nSELECT info -&gt;&gt; &#39;name&#39; AS customer FROM biz_orders;\n\n\n4、根据json对象的key查询值\nSELECT\n   info -&gt; &#39;items&#39; -&gt;&gt; &#39;product&#39; as product\nFROM\n   biz_orders\nORDER BY\n   product;\n\n\n5、where查询中使用json字段​​​​​​​\nSELECT\n   info -&gt;&gt; &#39;name&#39; AS customer\nFROM\n   biz_orders\nWHERE\n   info -&gt; &#39;items&#39; -&gt;&gt; &#39;product&#39; &#x3D; &#39;辣条&#39;\n\n\n6、case 查询​​​​​​​\nSELECT\n   info -&gt;&gt; &#39;name&#39; AS customer,\n   info -&gt; &#39;items&#39; -&gt;&gt; &#39;product&#39; AS product\nFROM\n   biz_orders\nWHERE\n   CAST (\n      info -&gt; &#39;items&#39; -&gt;&gt; &#39;qty&#39; AS INTEGER\n   ) &#x3D; 6\n\n​\n7、聚合函数​​​​​​​\nSELECT\n  MIN( CAST ( info -&gt; &#39;items&#39; -&gt;&gt; &#39;qty&#39; AS INTEGER ) ),\n  MAX( CAST ( info -&gt; &#39;items&#39; -&gt;&gt; &#39;qty&#39; AS INTEGER ) ),\n  SUM( CAST ( info -&gt; &#39;items&#39; -&gt;&gt; &#39;qty&#39; AS INTEGER ) ),\n  AVG( CAST ( info -&gt; &#39;items&#39; -&gt;&gt; &#39;qty&#39; AS INTEGER ) ) \nFROM\n  biz_orders;\n\n\n8、类型查询​​​​​​​\nSELECT\n  json_typeof ( info -&gt; &#39;items&#39; -&gt; &#39;qty&#39; ) \nFROM\n  biz_orders;\n\n\n总结通过以上例子，知道在pg数据库中如何存储json数据，并且掌握基本的查询，在查询条件中使用json，在聚合函数中使用。虽然，关系型数据库的强项不是在于json处理，而MongoDb或者Redis等NoSQL更适合做这类处理，但是在不引入一个新数据存储的情况下，利用现有架构解决生产问题。随着pg甚至mysql不断向前发展，相信未来对于json等数据支持会越来越好，性能也会越来越高。\n","slug":"postgre-jsonb","date":"2023-03-28T07:07:16.000Z","categories_index":"","tags_index":"postgreSQL","author_index":"Sh1mwww"},{"id":"4d40d05723fa24dd61a0704313888ecd","title":"PostgreSQL和MySQL性能比较","content":"简介​在 Arctype 社区里，我们回答了很多关于数据库性能的问题，尤其是 PostgreSQL 和 MySQL 这两个之间的性能问题。在管理数据库中，性能是一项至关重要而又复杂的任务。它可能受到配置、硬件、或者是操作系统的影响。PostgreSQL 和 MySQL 是否具有稳定性和兼容性取决于我们的硬件基础架构。\n并不是所有关系型数据库（RDBMS）都是一样的。 虽然 PostgreSQL 和 MySQL 有一些地方很相似，但是在不同的使用场景中，它们都有各自的性能优势。虽然在上篇文章中我们已经讨论了一些它们之间的基本差异，但在性能上还有许多差异值得我们讨论。\n在本文中，我们将讨论工作负载分析和运行的查询。然后，我们将进一步解释一些可以提高 MySQL 和 PostgreSQL 数据库的性能的基本配置。最后总结一下 MySQL 和 PostgreSQL 的一些关键区别。\n​\n目录\n如何衡量性能\n查询JSON的性能\n索引开销\n数据库复制和集群\n并发\n总结\n\n​\n如何衡量性能MySQL 尽管在读写操作混合使用时并发性很差，但是因其优秀的读取速度而备受好评。PostgreSQL（俗称 Postgres）表示自己是最先进的开源关系数据库，并且已开发为符合标准且功能丰富的数据库。\n以前，Postgres 的性能更加平衡，也就是说，读取通常比MySQL慢，但后来它得到了改进，现在可以更有效地写入大量数据，从而使并发处理更好。MySQL 和 Postgres 的最新版本略微消除了两个数据库之间的性能差异。\n在 MySQL 中使用旧的MyIsam引擎可以非常快速地读取数据。遗憾的是最新版本的 MySQL 并没有使用该引擎。但是，如果使用 InnoDB（允许键约束，事务），则差异可以忽略不计。InnoDB 中的功能对于企业或有很大用户量的应用程序至关重要，因此不能选择使用旧引擎。但是随着 MySQL 版本不断更新，这种差异越来越小。\n数据库基准测试是一个用于表现和比较数据库系统或这些系统上的算法的性能（时间，内存或质量）的可再现的实验框架。 这种实用的框架定义了被测系统、工作量、指标和实验。\n在接下来 4 节内容中，我们讨论一下每个数据库各自的性能优点。\n​\nJSON 查询在 Postgres 中更快在本节中，我们看下 PostgreSQL 和 MySQL 之间的基准测试的差异\n执行步骤\n创建一个项目（Java、 Node、或者Ruby），并且该项目的数据库使用的是 PostgreSQL 和 MySQL。\n创建一个 JSON 对象，然后执行读取和写入操作。\n整个 JSON 对象的大小为约为 14 MB，在数据库中创建约 200 至 210 个条目。\n\n统计数据PostgreSQL： 平均时间（毫秒）：写入：2279.25、读取：31.65、更新：26.26\n\nMySQL： 平均时间（毫秒）：写入：3501.05、读取：49.99、更新：62.45\n\n结论从上面的数据可以看出，PostgreSQL 在处理 JSON 时的性能要比 MySQL 更好，当然这也是 PostgreSQL 亮点之一。\n我们可以对数据库进行频繁的操作（读取、写入、更新）来了解其性能，然后选出最好的来用到你的项目上。通过上面的测试数据结果我们可以知道，尽管 MySQL 的速度比 PostgreSQL 要快，但也只是在某些特定条件下。\n​\n索引索引是所有数据库最重要的特性之一。数据库在查询数据时，有索引查询比没有索引查询快的多。但是，索引也会给数据库带来额外的开销，所有我们好刚要用在刀刃上，别瞎用。在没有索引的情况下，数据库在查找数据时会进行全文搜索（Full Text），也就是会从第一行开始一行一行的进行对比查找，这样的话数据量越多，查询的越慢。\nPostgreSQL 和 MySQL 都有一些处理索引的特定的方法：\n\nB-Tree索引： PostgreSQL 支持 B-Tree 索引和 Hash 索引。同时 PostgreSQL 还支持以下特性：\n表达式索引： 我们可以为表达式或函数来创建一个索引，而不是用字段。\n局部索引： 索引只是表的一部分\n\n假设 PostgreSQL 有一个 user 表，表的每一行代表一个用户。那么表可以这么定义：\nCREATE TABLE users (\n    id    SERIAL PRIMARY KEY,  \n    email VARCHAR DEFAULT NULL,  \n    name  VARCHAR\n);\n\n假设我们为该表创建如下索引：\n\n上面两个索引有什么区别呢？ 索引 #1 是一个局部索引，索引 #2 是一个表达式索引。 正如 PostgreSQL 文档所描述的那样，\n\n\n\n\n\n\n\n\n\n “局部索引建立在由条件表达式定义的表中的行子集上（称为局部索引的谓词）。索引仅包含满足谓词的那些表行的条目。使用局部索引的主要原因是避免索引常见的值。由于查询通常会出现的值（占所有表行的百分之几以上的值）无论如何都会遍历大多数表，因此使用索引的好处是微不足道的。更好的策略是创建局部索引，其中这些行完全排除在外。局部索引减少了索引的大小，因此加快了使用索引的查询的速度。 这也将使许多写入操作速度更快，因为索引不需要在所有情况下都更新。” —— 摘自PostGres文档\nMySQL： :MySQL 大部分索引（PRIMARY KEY、UNIQUE、INDEX、FULLTEXT）在使用时都是使用 B-Tree 数据结构。特殊情况下也会使用 R-Tree 的数据结构。 MySQL 也支持 Hash 索引，而且在 InnoDB 引擎下使用 FULLTEXT 索引时是倒序排列的。\n​\n数据库复制PostgreSQL 和 MySQL 的另一个性能差异是复制。复制指的是将数据从一个数据库复制到另外一台服务器上的数据库。这种数据的分布意味着用户现在可以访问数据而不直接影响其他用户。数据库复制最大的困难之一是协调整个分布式系统中的数据一致性。MySQL 和 PostgreSQL 为数据库复制提供了几个选项。除了一个主服务器，一个备用数据库和多个备用数据库之外，PostgreSQL 和MySQL 还提供以下复制选项：\n​\n多版本并发控制（MVCC）当用户同时对一个数据库进行读和写操作时，这种现象就叫并发现象。因此，多个客户端同时读取和写入会导致各种边缘情况&#x2F;竞赛条件，即，对于相同的记录X和许多其他条件，先读取后写入。各种现代数据库都利用事务来减轻并发问题。\nPostgres 是第一个推出多版本并发控制（MVCC）的 DBMS，这意味着读取永远不会阻止写入，反之亦然。此功能是企业偏爱 Postgres 而不是 MySQL 的主要原因之一\n\n\n\n\n\n\n\n\n\n “不同于大多数数据库使用锁来进行并发控制, Postgres通过使用多版本模型维护数据一致性。此外，在查询数据库时，无论基础数据的当前状态如何，每个事务都会像以前一样看到数据快照（数据库版本）。它可以防止事务查看同一数据行上的（其他）并发事务更新引起的不一致数据，从而为每个数据库会话提供事务隔离。” —— 摘自PostGres文档\nMVCC 允许多个读取器和写入器同时与 Postgres 数据库进行交互，从而避免了每次有人与数据进行交互时都需要读写锁的情况。附带的好处是此过程可显着提高效率。MySQL 利用 InnoDB 存储引擎，支持对同一行的写入和读取而不会互相干扰。MySQL每次将数据写入一行时，也会将一个条目写入回滚段中。此数据结构存储用于将行恢复到其先前状态的回滚日志。之所以称为回滚段，因为它是用来处理回滚事务的工具。\n\n\n\n\n\n\n\n\n\n “InnoDB 是一个多版本存储引擎：它保留有关已更改行的旧版本的信息，以支持诸如并发和回滚之类的事务功能。此信息存储在表空间中的数据结构中，该数据结构称为回滚段（Oracle 中也有类似的结构）。InnoDB 使用回滚段中的信息来执行事务回滚中所需的撤消操作。它还使用该信息来构建行的早期版本以实现一致的读取。” —— 摘自MySQL文档\n总结在本文中，我们处理了PostgreSQL和MySQL之间的一些性能差异。虽然数据库性能会受硬件、操作系统类型等等的影响，但是最主要的是你对目标数据库的了解。PostgreSQL 和 MySQL 都有各自的有点和缺点，但是了解哪些功能适合某个项目并整合这些功能最终可以提高性能。\n​\n","slug":"postgres-mysql","date":"2023-03-28T06:55:41.000Z","categories_index":"","tags_index":"mysql,postgreSQL","author_index":"Sh1mwww"},{"id":"a151887ec11ae6391c8864d43efe6e8e","title":"分布式和微服务的区别","content":"1.分布式和微服务有什么区别呢？分布式的核心就一个字：拆。只要是将一个项目拆分成了多个模块，并将这些模块分开部署，那就算是分布式。\n如何拆呢？有两种方式：水平拆分，或垂直拆分（也称为“横向拆分”和“垂直拆分”），具体如下：\n水平拆分：根据“分层”的思想进行拆分。例如，可以将一个项目根据“三层架构”拆分成 表示层（jsp+servlet）、业务逻辑层（service）和数据访问层（dao），然后再分开部署：把表示层部署在服务器A上，把service和dao层部署在服务器B上，然后服务器A和服务器B之间通过dubbo等RPC进行进行整合（在左下角的“阅读原文”里有dubbo的视频课程，可以点击学习），如图所示。\n垂直拆分：根据业务进行拆分。例如，可以根据业务逻辑，将“电商项目”拆分成“订单项目”、“用户项目”和“秒杀项目”。显然这三个拆分后的项目，仍然可以作为独立的项目使用。像这种拆分的方法，就成为垂直拆分。\n什么是微服务呢？\n从名字就能知道，“微服务”就是非常微小的服务。\n微服务可以理解为一种非常细粒度的垂直拆分。例如，以上“订单项目”本来就是垂直拆分后的子项目，但实际上“订单项目”还能进一步拆分为“购物项目”、“结算项目”和“售后项目”，如图。\n现在看图中的“订单项目”，它完全可以作为一个分布式项目的组成元素，但就不适合作为微服务的组成元素了（因为它还能再拆，而微服务应该是不能再拆的“微小”服务，类似于“原子性”）。\n总结：\n分布式：拆了就行。\n微服务：细粒度的垂直拆分。\n2.Java中不是有GC吗，怎么还有内存泄漏一说？答：Java内存有两种常见问题：内存溢出和内存泄漏。\n内存溢出好理解，就是JVM内存有限。如果对象太多，JVM内存放不下了，就会内存溢出。\n那什么是内存泄漏？首先得明确，GC只会回收那些“不可达”的对象（可以简单理解为，如果一个对象存在着指向它的引用，这个对象就“可达”；如果没有引用指向它，则“不可达”）。\n若一个对象是“无用但可达的”，就会造成内存泄漏。\n如下代码中，obj的值是null，因此是“无用的”；但同时obj又同时被被list引用，因此是“可达”的，所以此时的obj就会造成内存泄漏。\nObject obj &#x3D; new Object();\n\nlist.add( obj );\nobj &#x3D; null ;\n除了上面obj这种内存泄漏的情况以外，在实际开发中最常见的内存泄漏就是打开资源后没有调用close()方法。例如socket、io流等，都需要再最后close()一下防止内存泄漏。\n","slug":"attribute","date":"2023-03-28T06:20:55.000Z","categories_index":"","tags_index":"service","author_index":"Sh1mwww"},{"id":"e59b750bc82711213370e958d83d7a80","title":"Nginx攻击方式和解决方案","content":"3分钟了解 NginxNginx是一款高性能的Web服务器和反向代理服务器。\n它可以用来搭建网站、做应用服务器，能够处理大量的并发连接和请求。\n\n静态内容托管（主要）：可以用来做网页、图片、文件的 “静态”内容托管。\n动态内容托管（主要）：将经常访问的动态内容缓存到内存中，提高访问速度和性能。\n反向代理（主要）：将客户端的请求发送到后端真实服务器，并将后端服务器的响应返回给客户端。\n\n*类似于一个快递收发室，指挥快递（流量）应该投递到哪个买家。\n它还能提供一些高级功能：\n\n负载均衡：将客户端的请求分发到多个后端服务器上，从而提高服务的可用性和性能。\nSSL&#x2F;TLS加密传输：通过加密和认证保护数据传输安全。\nHTTP&#x2F;2支持：通过多路复用技术提高并发连接处理能力和页面加载速度。\n安全防护：提供多种防护机制，如限制IP访问、请求频率限制、反爬虫等。\n动态内容处理：支持FastCGI、uWSGI等协议，与后端应用服务器进行动态内容交互。\n日志记录：记录访问日志和错误日志，方便监控和排查问题。\n自定义模块开发：支持自定义模块开发，可以根据需求进行二次开发和扩展。读到这里，我知道很多人脑子都要爆了。现在让我们直入主题。结合以上功能的能做哪些攻击方式。\n\n反向代理攻击使用Nginx作为反向代理服务器，将攻击流量转发到目标服务器。这样就能隐藏攻击流量的真实地址。\nserver &#123;\n    listen 80;\n    server_name www.example.com;\n    location &#x2F; &#123;\n        proxy_pass http:&#x2F;&#x2F;backend_server;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    &#125;\n&#125;\n\n所有访问www.example.com:80的流量全部都会转发到http://backend_server服务器上。\nproxy_set_header X-Real-IP $remote_addr; 设置请求头提供真实来源ip。\nproxy_set_header Host $host;设置访问的Host。\n\n只要把X-Real-IP改成其他不存在的IP，就可以隐藏自己的真实IP地址，让攻击更难以被追踪和防御。当然相对于客户端来说，只能知道nginx的地址就不知道真实服务器的地址了。\nDDoS攻击DDoS攻击就是借助某些工具瞬间发动大量的请求，让服务器资源耗尽，无法正常响应其他用户的请求，一般也常用于压力测试。介绍一些常用的工具：\n\n**ApacheBench (ab)**：常用的命令行工具，用于模拟多个并发请求。可以控制请求总数、并发数等参数。\nSiege：命令行工具，和上面一样，并且还支持 HTTP和HTTPS协议。\nJMeter：一个功能强大的Java应用程序，可以用于模拟各种负载情况.JMeter可以通过图形界面进行配置，支持更多协议和数据格式，包括 HTTP、HTTPS、SOAP、REST 等。但事实往往比这个残酷，攻击者会做一些病毒，在网络上传播开来，病毒运行时可以直接疯狂访问服务器，或者利用Nginx提供的反向代理和其支持的比如socket、SSL，不断的建立握手请求。\n\n限流、黑名单防御主要给大家介绍怎么防御。这种病毒感染方式就不说了，我害怕戴银手铐。\nhttp &#123;\n    limit_req_zone $binary_remote_addr zone&#x3D;one:10m rate&#x3D;5r&#x2F;s;\n\n    geo $block &#123;\n        default 0;\n        include &#x2F;path&#x2F;to&#x2F;block_ip.txt;\n    &#125;\n\n    server &#123;\n        listen 80;\n\n        location &#x2F; &#123;\n            limit_req zone&#x3D;one burst&#x3D;10 nodelay;\n            if ($block) &#123;\n                return 403;\n            &#125;\n            proxy_pass http:&#x2F;&#x2F;backend;\n        &#125;\n    &#125;\n&#125;\n\nlimit_req_zone定义了一个名为“one”的限制请求速率的区域，该区域的大小为10MB，请求速率限制为每秒5个请求。\nlimit_req指定使用名为“one”的限制规则。\ngeo $block是黑名单，这个文件可以写需要屏蔽的ip。\nserver块中的location指令使用了limit_req和if表示黑名单的返回403状态码。\n\n负载均衡防御假设我有两个后端服务器。\nhttp &#123;\n  upstream backend &#123;\n    # 轮询方式的负载均衡\n    server backend1.example.com;\n    server backend2.example.com;\n  &#125;\n...\n  server&#123;...&#125;\n&#125;\n有多种负载均衡方式。\nserver &#123;\n  ...\n   location &#x2F;api&#x2F; &#123;\n     # 轮训\n     proxy_pass http:&#x2F;&#x2F;backend;\n   &#125;\n\n   location &#x2F;lb&#x2F; &#123;\n     # IP哈希方式的负载均衡\n     ip_hash;\n     proxy_pass http:&#x2F;&#x2F;backend;\n   &#125;\n\n   location &#x2F;upstream&#x2F; &#123;\n     # 根据服务器性能或响应时间进行加权轮询\n     upstream backend &#123;\n       server backend1.example.com weight&#x3D;2;\n       server backend2.example.com;\n     &#125;\n     # 对 backend 进行访问\n     proxy_pass http:&#x2F;&#x2F;backend;\n   &#125;\n\n   location &#x2F;least_conn&#x2F; &#123;\n     # 最少连接数的负载均衡\n     least_conn;\n     proxy_pass http:&#x2F;&#x2F;backend;\n   &#125;\n\n   location &#x2F;random&#x2F; &#123;\n     # 随机方式的负载均衡\n     random;\n     proxy_pass http:&#x2F;&#x2F;backend;\n   &#125;\n\n   location &#x2F;sticky&#x2F; &#123;\n     # 基于客户端IP的哈希方式的负载均衡\n     hash $remote_addr consistent;\n     server backend1.example.com;\n     server backend2.example.com;\n   &#125;\n &#125;\n很多人学nginx都会对ip_hash和基于客户端IP的哈希方式的负载均衡有疑惑。分不清，我一句话给大家讲清楚。\n\nip_hash能保证相同来源一定能访问相同的服务器，适用于登录等有状态的场景。在请求量少的时候，容易出现很多ip落在同一服务器上，分布不均衡。\n基于客户端ip的hash，是根据客户端 IP 地址计算哈希值，然后将哈希值与后端服务器数量取模。使请求平均分配到不同的服务器上，也能保证同一ip请求落到同一服务器上。但是可以保证各个服务器比较均衡。我认为使用方式二更好，可能理解有限，欢迎各位读者分享自己的看法！\n\n网络钓鱼攻击黑客可以使用Nginx伪装成一个合法的网站，诱骗用户输入敏感信息。例如，他们可以使用Nginx构造一个伪造的登录页面，让用户输入用户名和密码，然后将这些信息发送给黑客服务器。\n其实就是静态托管+反向代理功能的组合。\nserver &#123;\n    listen       80;\n    server_name  example.com;\n\n    # 静态网站托管\n    location &#x2F; &#123;\n        root   &#x2F;var&#x2F;www&#x2F;mywebsite&#x2F;dist;\n        index  index.html index.htm;\n    &#125;\n\n    # API代理转发\n    location &#x2F;api &#123;\n        proxy_pass  http:&#x2F;&#x2F;localhost:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    &#125;\n&#125;\n访问根目录就访问到静态的网站资源。访问&#x2F;api路由转发到api服务上。\n","slug":"nginx-attact","date":"2023-03-28T05:46:26.000Z","categories_index":"","tags_index":"nginx","author_index":"Sh1mwww"},{"id":"6d1c99c12c344c631fb3ed115b7df580","title":"如何使用Docker部署一个go程序","content":"Docker定义Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows操作系统的机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。\n本文我们介绍怎么使用 Docker 部署 Go 项目。阅读本文，需要读者朋友们了解 Docker 的基本操作。\ngo项目的开发首先，我们开发一个简单的 Go Web 项目，使用 Go 内置命令行工具 go build 编译生成可执行文件 .&#x2F;hello。这是项目结构\n.\n├── Dockerfile\n├── go.mod\n├── hello\n├── main.go\n└── service.log\n我们使用 Go 标准库编写一个 Web 项目，运行编译生成的可执行程序，访问 http://127.0.0.1:8080/hello，输出 hello word。\ncurl http:&#x2F;&#x2F;127.0.0.1:8080&#x2F;hello\n# 输出\nhello world\nDocker的使用我们开始编写 Dockerfile 文件，使该项目可以支持使用 Docker 部署。\nDockerfile 文件：\n# 基础镜像\nFROM alpine:3.12\n# 维护者\nMAINTAINER frank\n# docker build 时执行命令 - 创建目录\nRUN mkdir -p &quot;&#x2F;data&#x2F;app&quot; \\\n&amp;&amp; ln -sf &#x2F;dev&#x2F;stdout &#x2F;data&#x2F;app&#x2F;service.log\n# 工作目录\nWORKDIR &quot;&#x2F;data&#x2F;app&quot;\n# 拷贝\nCOPY hello &#x2F;data&#x2F;app&#x2F;hello\n# docker run 时执行命令\nENTRYPOINT [&quot;.&#x2F;hello&quot;]\n在编写完 Dockerfile 文件之后，我们可以使用 docker 命令构建镜像，前提是我们本机已安装 Docker。\ndocker build -t hello:v1.0.0 .\n运行以上构建 Docker 镜像的命令之后，我们就已成功构建 Docker 镜像。\n","slug":"docker-go","date":"2023-03-12T07:30:59.000Z","categories_index":"","tags_index":"docker,golang","author_index":"Sh1mwww"},{"id":"5af57275e38843538bce4be83af3cccf","title":"理解 ES 查询机制","content":"为什么需要使用 ES 进行搜索ES除了拥有索引上的优势，最重要的还是数据的结构，这都是ES为什么效率高，会使用它的原因。\n1，结构化数据 VS 非结构化数据\n结构化数据：  也称作行数据，关系型数据库进行存储和管理,是由二维表结构来逻辑表达和实现(可以使用行、列来表现)的数据，严格地遵循数据格式与长度规范。\n非结构化数据：  又可称为全文数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、word文档，邮件，各类报表、图片和音频、视频信息等。\n\n其他的不同之处还有：\n\n结构化数据往往占用的空间较小，占企业数据的 20% 左右，容易管理。\n非结构化数据通常占用更多的存储空间，约占企业数据的 80% 左右，比较难以管理\n\n2，结构化搜索 vs 全文搜索\n结构化搜索：  通常查询具有固有结构的数据，答案要么是肯定的，要么是否定的（即便是类似正则匹配这样的结构化搜索，正则表达式匹配数据也是确定的），数据要么属于查询结果集合，要么不属于。\n全文搜索：  通常查询全文字段&#x2F;文档的所有内容，答案返回的是一系列可能的数据，数据有一定概率属于结果集合。\n\n到这里，为什么需要使用 ES 进行搜索的答案就很明确了：对于非结构化文本（比如评论内容），传统的结构化搜索难以满足需求，于是就会使用 ES 进行全文搜索。当然 ES 不仅可以进行全文搜索，也可以进行一部分的结构化搜索，更加扩大了他的应用范围。对于数据量巨大的情景，有公司会使用 ES 代替传统的 MySQL 管理数据。\nES 基本概念介绍本小结主要是介绍 ES 的一些基本概念，目的是方便之前没有了解过 ES 的同学可以理解这次分享所介绍的内容。\n1，ES 存储模型ES 在设计存储模型时，考虑了大家从关系型数据库转换肯能带来的困难，于是设计了 Index、Type、Document、Field 分别于对应传统关系型数据库(比如 MySQL) 的 Database、Table、Row、Column。注意： ES 存储时，并没有 Type 的概念，同一个Index 里的 Type 会拍平存储，只是方便理解才会对使用者提供这样一个抽象。由于Type 的存在会带来一些问题，在后续的版本里会逐步移除。\n2，ES 与 LuceneES 底层基于 Lucene 开发，Lucene作为其核心来实现索引和搜索的功能。我们虽然讲的是 ES，但很大一部分内容是 Lucene 的实现。​\n","slug":"es-search","date":"2023-03-12T07:19:46.000Z","categories_index":"","tags_index":"elasticsearch","author_index":"Sh1mwww"},{"id":"bee84e518ec4344cb23b6f3015f64773","title":"Goroutine基础","content":"这篇文章将关注 Go 语言基础部分。我们将讨论关于性能方面的一些知识，并通过创建一些简单的 goroutine 来扩展我们的应用程序。\n我们还会关注一些 Go 语言的底层执行逻辑以及 Go 语言与其他语言的不同之处。\nGo 语言的并发继续讨论之前，我们必须理解并发与并行的概念。Golang 可以实现并发和并行。\n我们一起来看下并发与并行的区别。\n理解并发应用程序可能会通过处理多个进程来完成预期的功能。我们来假设一个简单的电子商务网站，经评估有下列需要并发执行的任务：\n\n在网页的顶部显示最新的交易和产品信息；\n显示网站当前的在线用户数量；\n当用户选择商品之后更新购物车详情；\n为“目标交易额”倒计时；\n\n该网站需要同时运行所有这些任务，以使用户与网站保持关联，并使网站对用户有吸引力并吸引更多业务。\n因此，为了满足业务需要，一个简单的应用程序或者网站都可能包含一组后台运行的任务。\n上图所示的两个示例中，有多个任务同时执行，但是它们之间仍然有区别。让我们进一步研究以便能更了解。\n理解并发与并行执行\n\n处理并发应用假设这样一种场景，我们有一台单核机器，需要完成多个任务，但有个限制，在任何时刻，单核机器上只能运行一个任务。\n在并发模型中，任务之间存在上下文切换。该程序正在处理多个任务，但由于我们只有单核，因此任务无法一起执行。\n任务之间的上下文切换很快，以至于我们感觉任务是同时运行的。\n在执行过程中没有并行执行的因素，因为是一个单核系统，多进程不能并行执行。\n如上图所示，Concurrency (Without Parallelism) 有两个任务需要并发执行。在任何时候，只有一个任务在运行并且任务之间存在上下文切换。\n应用程序加入并行使用单核的情况下，存在核数限制。如果我们给机器增加核数，就可以在不同的内核上同时执行任务。\n在上图中(Parallelism)，任一时刻都有两个任务在执行，这两个任务运行在不同的内核上。\n并发是某一时间段内同时处理多个任务，并行是在某一时间点能执行多个任务。\n使用 Go 语言可以轻松地将程序从并发扩展为并行执行。\n使用协程使用 Go 语言实现并发和并行，我们需要了解协程(Goroutines)的概念。Go 语言的协程可以理解为线程之上的一个包装器，由 Go 运行时管理而不是操作系统。\nGo 运行时负责给协程分配和回收资源，协程与完成多任务的线程非常相似但又比操作系统线程消耗更少的资源。协程与线程之间并非一对一的关系。\n我们可以将应用程序“拆解”成多个并发任务，这些任务可以由不同的 goroutine 完成，通过这种方式即可实现了 Go 语言并发。\n协程的优点：\n\n更轻量级；\n易扩展；\n虚拟线程；\n需要更少的初始内存(2KB)；\n有必要的话，Go 运行时能分配更多的内存；一起来看下一个简单的例子：package main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nfunc main() &#123;\n  start :&#x3D; time.Now()\n  func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  elapsedTime :&#x3D; time.Since(start)\n\n  fmt.Println(&quot;Total Time For Execution: &quot; + elapsedTime.String())\n\n  time.Sleep(time.Second)\n&#125;\n上面的代码按顺序依次在 main 函数里面执行了两个独立函数。\n\n代码没有使用协程，程序在同一个线程中执行完成。程序没有任何的并发性，执行结果如下：代码按顺序执行，从主函数开始，先执行第一个函数，再执行第二个函数，最后从主函数正常退出。\n引入协程上面的场景例子中没有使用任何的协程。我们可以在执行函数之前使用 go 关键字开启协程。\n依旧是上面的例子，我们一起来看看使用 go 关键字开启协程之后会是什么样的：\npackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nfunc main() &#123;\n  start :&#x3D; time.Now()\n  go func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  go func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  elapsedTime :&#x3D; time.Since(start)\n\n  fmt.Println(&quot;Total Time For Execution: &quot; + elapsedTime.String())\n\n  time.Sleep(time.Second)\n&#125;\n执行上面的代码输出：\n上面的代码，使用 go 关键字分别开启了两个协程并执行各自的函数，包括主协程，总共有 3 个协程。\n理解与顺序执行的不同上面的代码，我们使用 go 关键字开启协程，函数会在协程中完成执行，而不是在主协程中执行，这样增加了并发并提高了程序性能。\n增加并行性\n\nGo 语言里，可以通过下面这行简单的代码设置程序运行的内核数目(PS:从 Go 1.5开始，Go 的 GOMAXPROCS 默认值已经设置为 CPU 的核数)。\nruntime.GOMAXPROCS(4)\n这可以指定程序在多核机器上运行，上面一行代码指定程序可以使用四个内核来执行。\n一旦创建了协程，便可以在不同的内核中执行，从而实现并行并加快程序执行速度。\npackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n    &quot;runtime&quot;\n)\n\nfunc main() &#123;\n  runtime.GOMAXPROCS(4)\n  start :&#x3D; time.Now()\n  go func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  go func() &#123;\n    for i:&#x3D;0; i &lt; 3; i++ &#123;\n      fmt.Println(i)\n    &#125;\n  &#125;()\n\n  elapsedTime :&#x3D; time.Since(start)\n\n  fmt.Println(&quot;Total Time For Execution: &quot; + elapsedTime.String())\n\n  time.Sleep(time.Second)\n&#125;\n上面的代码输出如下：使用 Go 语言可以轻松实现并发和并行。只需在函数之前添加 go 关键字就可以提高程序执行速度。\n","slug":"Goroutine","date":"2023-03-12T07:01:19.000Z","categories_index":"","tags_index":"golang","author_index":"Sh1mwww"},{"id":"30d936a7afc35294893d61738d3f7081","title":"Springboot 实现 ES-SQL 的流程","content":"背景记录一个最近半年犯了两次的低级编码错误，校验某个字符串信息为枚举类的某实例时，写成了：枚举类的实例.equals(字符串) ，结果总是 false  ，打印信息貌似正确，实际执行结果总是不达预期，仔细看看代码才发现问题。本文记录本周开发工作中遇到的几个小问题，Bug 是无法完全消除的，只能尽量减少。\nES 连接及时关闭查看某进程的端口占用时，看到好多正连接着的 ES 连接对象，普通的 Java Web 应用，没有后台任务，怎么会保持这么多连接呢？\n问题分析：代码有缺陷，有些请求使用 ES 进行数据查询完成后，没有关闭 ES 连接对象。ES 连接不关闭，有什么问题呢？\nLinux 的每个连接都会创建一个文件句柄，毫无疑问，Socket 连接用完后不关闭，会导致端口资源泄漏。\njackson 序列化异常在将 ElasticSearch 的 Response 对象序列化时碰到一个异常信息：\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class org.elasticsearch.common.text.Text and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.elasticsearch.action.search.SearchResponse[&quot;hits&quot;]-&gt;org.elasticsearch.search.SearchHits[&quot;hits&quot;]-&gt;org.elasticsearch.search.SearchHit[0]-&gt;org.elasticsearch.search.SearchHit[&quot;shard&quot;]-&gt;org.elasticsearch.search.SearchShardTarget[&quot;nodeIdText&quot;])\n有两种解决办法：\n\n定义一个配置类，设置 jackson 的序列化配置属性。\n通过 SpringBoot 的全局配置 spring.jackson.serialization.FAIL_ON_EMPTY_BEANS 设置为 false ，该配置默认为 true，这个方式更方便。\n\nElasticSearch 执行 SQL 的 Java 实现回到文章标题说的问题，ElasticSearch 6 以后的版本支持 SQL 语句检索了，如何用 Java 代码实现 ES SQL 检索呢？\n第一 Part，基础知识。先搞明白需求及相关的技术支持，主要如下：\n\nElasticSearch-SQL 功能，区分 GitHub 上的一个插件和 ElasticSearch 自身的支持能力。ES 6 以后内置了X-Pack 组件，提供了  Elasticsearch SQL 能力，就是说不用安装插件就可以使用 ES SQL 能力了。而网上很多都是介绍 ElasticSearch SQL 插件安装的，却不曾想 ES 已经内置了。\nES 6 与 ES 8 的 Rest API 的语法不一样，8 以上的版本语句是 /_xpack/_sql?format= ，但是旧版本是 /_xpack/sql?format= ，版本依赖问题有时候挺坑的。网上大量的资料，估计都是来自官网，全都是 /_xpack/_sql?format=，结果我测试用的环境是 ES6 的，一直报错。\n\n第二 Part， Java 实现 ES-SQL 操作的几种方法：\n\nRest API 请求 /_xpack/sql?format=。\n*JDBC-ES ，这个功能是收费的。\n\n第三 Part，使用 elasticsearch-rest-high-level-client 包的 RestAPI 客户端工具可以实现 ES-SQL 的操作。基本思路是构建一个 RestClient 对象，请求路径为   /_xpack/_sql?format=json 这个用 txt 的时候，总是得不到结果，用 json 就没问题。\n重要源代码如下：\n&#x2F;&#x2F; ES 连接信息构造\nRestClientBuilder restClientBuilder  &#x3D; null;\nif (hasPwd) &#123;\n\tfinal CredentialsProvider credentialsProvider &#x3D; new BasicCredentialsProvider();\n\t\n\t&#x2F;** 设置 ES 认证信息 *&#x2F;\n    credentialsProvider.setCredentials(AuthScope.ANY,new UsernamePasswordCredentials(userName, password));\n\trestClientBuilder &#x3D; RestClient.builder(https)\n                    .setHttpClientConfigCallback(httpAsyncClientBuilder -&gt; httpAsyncClientBuilder.setDefaultCredentialsProvider(credentialsProvider));\n&#125; else &#123;\n  \trestClientBuilder &#x3D; RestClient.builder(https)\n&#125;\n\n&#x2F;&#x2F; xpack-sql 请求构造\nString searchIndex &#x3D; &quot;&#x2F;_xpack&#x2F;sql?format&#x3D;json&quot;;\nRequest request &#x3D; new Request(&quot;POST&quot;, searchIndex);\nrequest.setJsonEntity(queryJsonString);\n\nRestClient restClient &#x3D; restClientBuilder.build();\ntry &#123;\n    Response response &#x3D; restClient.performRequest(request);\n    String body &#x3D; EntityUtils.toString(response.getEntity());\n    &#x2F;&#x2F; TODO 处理 ES 响应结果\n&#125; catch (IOException e) &#123;\n&#125;\n\nToDesk 服务占据高 CPU电脑一直占据大量的 CPU，top 看是 ToDesk 进程，执行下面的操作好了：\nsudo launchctl unload &#x2F;Library&#x2F;LaunchDaemons&#x2F;com.youqu.todesk.service.plist\n我碰到的问题，绝对不是个例，百度是个好东西，面向百度编程也没什么不好啊！\n","slug":"SpringBootES","date":"2023-03-11T09:47:46.000Z","categories_index":"","tags_index":"Springboot,elasticsearch","author_index":"Sh1mwww"},{"id":"4783a03c76b99864e7fb550cfa766073","title":"Git Flow 工作原理","content":"一、 Git Flow 工作模型的原理无规矩不成方圆，但是规矩太多了，则感觉到束缚。我们一个人工作的时候喜欢无拘无束，想怎么干就怎么干，没有人评判，没有人检验。时间久了就会盲目自大，以为增删改查熟悉业务就能够搞定一些。但是当项目逐渐扩大，原来的灵活逐渐变成了混乱，原来的快速迭代因为过于随意的代码，而开发进度迟迟不前。掌握一种规范，便在处理类似问题的时候有章可循，也能够快速的融入一个团队。另外所谓规范，可以说是比较好的实践，按照规范来，项目也能稳健的发展。\nGit Flow 就是如何使用git 分支的一种规范，或者叫做推荐。\n根据Git Flow 的推荐，我们要将Git 的分支分为 master 、develop 、hotfix 、release、feature这五个分支。各种分支分别负责不同的功能，平时开发的时候各司其职，因此会有比较小的冲突率。那么可以用这些减少冲突的时间，少加会班，多有点自己的生活岂不快哉。一图胜千言：\n\n\nmaster 分支master 分支主要方稳定、随时可上线的版本。这个分支只能从别的分支上合并过来，一般来讲，从develop 上合并，或者从bugfix 分支上合并过来。不能直接在master 分支上进行commit文件。因为是稳定的版本，所以每次版本发布都要在这个分支上添加标签(tag)。\ndevelop 分支develop 分支是所有开发分支的母体，所有的开发分支都要从develop上切出来，开发完成之后最后都要合并到develop上。\nhotfix 分支hotfix 分支用来修复生产中的紧急bug，由于develop分支尚处于开发过程中，代码不稳定，不能直接应用于生产。所以从master分支上切出一个分支，修复完成之后合并到master分支，并且合并到develop上。\nrelease 分支release 分支可以称之为预发布的版本。当我们认为develop版本的代码已经趋于成熟，我们可以打一个release分支。在release 分支上测试完成之后，要将代码合并到master分支和develop上。master 分支是线上版本，而合并到develop版本是因为，在测试过程中，一些细节的东西可能会修改，因此这些优化的内容也应该合并到最终版本以及开发版本中。\nfeature 分支feature 分支是最经常使用的分支了。当我们收到一个新的开发功能时，应该在develop分支上切出一个feature分支。用来完成新功能的开发，开发完成之后，要合并进develop分支上。\n二、 Git Flow 工具的使用基本上各种git的客户端软件都会支持Git Flow 工作模型。sourcetree 上使用git flow 工作模型就很流畅，体验很好。但是为了全平台上通用，以及理解原理，快速上手。我们来学习下Git Flow 的命令行操作。\ngit flow 是一种git的使用规范，当然也有相应的工具集，命令行命令让我们使用。\n\n\n1、起步安装git flow\nbrew install git-flow-avh\n初始化git flow 工具库\ngit flow init\n之后都按照默认的去配置，直接按enter键继续。\n2、feature 分支操作增加feature新特性分支\ngit flow feature start your roverliang&#x2F;addlist\n示例:\nroverliang$ git flow feature start roverliang&#x2F;addlist\nSwitched to a new branch &#39;feature&#x2F;roverliang&#x2F;addlist&#39;\n\nSummary of actions:\n- A new branch &#39;feature&#x2F;roverliang&#x2F;addlist&#39; was created, based on &#39;develop&#39;\n- You are now on branch &#39;feature&#x2F;roverliang&#x2F;addlist&#39;\n\nNow, start committing on your feature. When done, use:\n\n     git flow feature finish roverliang&#x2F;addlist\n\nroverliang$ git branch\n  develop\n* feature&#x2F;roverliang&#x2F;addlist\n  master\n完成新特性这个动作执行的是下面的流程:\n\n合并 addlist 分支到 develop\n删除这个新特性分支\n切换回 develop 分支git flow feature finish roverliang&#x2F;addlist\n\n示例：\nroverliang$ git flow feature finish roverliang&#x2F;addlist\nSwitched to branch &#39;develop&#39;\nYour branch is up to date with &#39;origin&#x2F;develop&#39;.\nAlready up to date.\nDeleted branch feature&#x2F;roverliang&#x2F;addlist (was 2e1b475).\n\nSummary of actions:\n- The feature branch &#39;feature&#x2F;roverliang&#x2F;addlist&#39; was merged into &#39;develop&#39;\n- Feature branch &#39;feature&#x2F;roverliang&#x2F;addlist&#39; has been locally deleted\n- You are now on branch &#39;develop&#39;\n\nroverliang$ git branch\n* develop\n  master\nroverliang$\n获取一个发布的新特性的分支\ngit flow feature track origin MYFEATURE\n\n3、release 分支操作准备release 版本\ngit flow release start RELEASE [BASE]\n\n你可以选择提供一个 [BASE]参数，即提交记录的 sha-1 hash 值，来开启动 release 分支. 这个提交记录的 sha-1 hash 值必须是&#39;develop&#39; 分支下的。\n\n示例：\nroverliang$ git branch\n  develop\n  feature&#x2F;test\n* master\nroverliang$ git log --pretty&#x3D;oneline -3\n2e1b475f9825275aefa0892cfe5259aaac9a3483 (HEAD -&gt; master, origin&#x2F;test2, origin&#x2F;master, origin&#x2F;feature&#x2F;test, origin&#x2F;develop, feature&#x2F;test, develop) delte some content\n2d22f306d2dca363b8aaa05743be342a505aabb0        renamed:    demo.txt -&gt; test.txt\nfbf025e210952c3cdb10e219c4ee5f82b9f36327        modified:   demo.txt\nroverliang$\n\n发布release 版本\ngit flow release track RELEASE\n\n完成release 版本 相当于执行以下几个动作：\n\n归并 release 分支到 ‘master’ 分支\n用 release 分支名打 Tag\n归并 release 分支到 ‘develop’\n移除 release 分支git flow release finish RELEASE\n\n4、bugfix 分支操作紧急修复的需求：\n\n紧急修复来自这样的需求：生产环境的版本处于一个不预期状态，需要立即修正。\n有可能是需要修正 master 分支上某个 TAG 标记的生产版本。\n\n开始紧急修复，开启hotfix 分支\ngit flow hotfix start VERSION [BASENAME]\nVERSION 参数标记着修正版本。你可以从 [BASENAME]开始，[BASENAME]为finish release时填写的版本号\n\n完成紧急修复:当完成紧急修复分支，代码归并回 develop 和 master 分支。相应地，master 分支打上修正版本的 TAG。\ngit flow hotfix finish VERSION","slug":"gitflow intro","date":"2023-03-11T08:13:35.000Z","categories_index":"","tags_index":"gitflow","author_index":"Sh1mwww"},{"id":"90bb244b78e2bf4cc75c96413958b4eb","title":"ElasticSearch数据库简单介绍","content":"elasticsearch简介ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。\nelasticSearch的使用场景1、为用户提供按关键字查询的全文搜索功能。2、实现企业海量数据的处理分析的解决方案。大数据领域的重要一份子，如著名的ELK框架(ElasticSearch,Logstash,Kibana)。\n与其他数据存储进行比较\n\nelasticsearch的特点\n天然分片，天然集群  es 把数据分成多个shard，下图中的P0-P2，多个shard可以组成一份完整的数据，这些shard可以分布在集群中的各个机器节点中。随着数据的不断增加，集群可以增加多个分片，把多个分片放到多个机子上，已达到负载均衡，横向扩展。在实际运算过程中，每个查询任务提交到某一个节点，该节点必须负责将数据进行整理汇聚，再返回给客户端，也就是一个简单的节点上进行Map计算，在一个固定的节点上进行Reduces得到最终结果向客户端返回。\n\n天然索引  ES 所有数据都是默认进行索引的，这点和mysql正好相反，mysql是默认不加索引，要加索引必须特别说明，ES只有不加索引才需要说明。而ES使用的是倒排索引和Mysql的B+Tree索引不同。\n\n\n传统关系性数据库\n弊端：\n\n对于传统的关系性数据库对于关键词的查询，只能逐字逐行的匹配，性能非常差。\n*匹配方式不合理，比如搜索“小密手机” ，如果用like进行匹配， 根本匹配不到。但是考虑使用者的用户体验的话，除了完全匹配的记录，还应该显示一部分近似匹配的记录，至少应该匹配到“手机”。倒排索引是怎么处理的全文搜索引擎目前主流的索引技术就是倒排索引的方式。传统的保存数据的方式都是记录→单词而倒排索引的保存数据的方式是单词→记录\n\n索引结构对比可以看到 lucene 为倒排索引(Term Dictionary)部分又增加一层Term Index结构，用于快速定位，而这Term Index是缓存在内存中的，但mysql的B+tree不在内存中，所以整体来看ES速度更快，但同时也更消耗资源（内存、磁盘）。\nlucene与elasticsearch的关系咱们之前讲的处理分词，构建倒排索引，等等，都是这个叫lucene的做的。那么能不能说这个lucene就是搜索引擎呢？\n还不能。lucene只是一个提供全文搜索功能类库的核心工具包，而真正使用它还需要一个完善的服务框架搭建起来的应用。\n好比lucene是类似于发动机，而搜索引擎软件（ES,Solr）就是汽车。\n目前市面上流行的搜索引擎软件，主流的就两款，elasticsearch和solr,这两款都是基于lucene的搭建的，可以独立部署启动的搜索引擎服务软件。由于内核相同，所以两者除了服务器安装、部署、管理、集群以外，对于数据的操作，修改、添加、保存、查询等等都十分类似。就好像都是支持sql语言的两种数据库软件。只要学会其中一个另一个很容易上手。\n从实际企业使用情况来看，elasticSearch的市场份额逐步在取代solr，国内百度、京东、新浪都是基于elasticSearch实现的搜索功能。国外就更多了 像维基百科、GitHub、Stack Overflow等等也都是基于ES的。\n","slug":"elk","date":"2023-03-05T07:57:17.000Z","categories_index":"","tags_index":"elasticsearch","author_index":"Sh1mwww"},{"id":"d4105a7b8a6b6b0d8864aefbdb368e9a","title":"mylover","content":"tags:    mylover兴趣广泛，喜欢篮球足球hiphop\n","slug":"mylover","date":"2022-04-15T07:42:17.000Z","categories_index":"","tags_index":"","author_index":"Sh1mwww"},{"id":"bccecdef4d85db8990deae967274cbf8","title":"myfirstblog","content":"简单的简介这是我的第一个blog，一个简单的hexo框架构成的网站\nSh1mwww本人很懒。不会定期更新使用","slug":"myfirstblog","date":"2022-04-15T07:41:44.855Z","categories_index":"","tags_index":"","author_index":"Sh1mwww"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2022-04-04T08:45:44.257Z","categories_index":"","tags_index":"","author_index":"Sh1mwww"}]